[
  {
    "objectID": "Project3.html",
    "href": "Project3.html",
    "title": "Apple Interview Question!",
    "section": "",
    "text": "Inspiration\nI once heard of a Apple interview question in which the candidate is given 100 coins (90 with heads facing up and 10 with tails facing up) and then asked to separate them into two piles of any size such that each pile has the same number of coins with the tails side facing up. Sounds simple enough, but there’s a catch… the candidate was to be blindfolded the entire time. This means they never saw the coins and have no idea which 10 coins are on tails or which 90 coins are on heads.\nThere is an algorithmic approach to this problem such that you can guarantee your answer will be true, but I was not able to come up with it. The best I could think to do is to randomly perform 100 coin flips and then separate my flipped coins into two piles of 50. I would then just have to hope that each pile has the same number of tails facing up.\nMotivating Question\nAlthough my answer is not exactly what the interviewers would have in mind, I have always been curious as to how often my answer would end up being correct merely by chance. Until now, I haven’t been able to figure out a way to answer this question without doing long mathematical calculations, and the free version of ChatGPT seems to give me a different answer each time I ask it, so I’m very excited to finally approximate the answer using this simulation.\nResults\n\nlibrary(purrr)\ncoin_flip &lt;- c(0, 1)\n\nflip_coins &lt;- function(num_coins) {\n  left_pile &lt;- sample(coin_flip, size = num_coins, replace = TRUE)\n  right_pile &lt;- sample(coin_flip, size = num_coins, replace = TRUE)\n  \n  tails_match &lt;- ifelse(sum(left_pile) == sum(right_pile), 1, 0)\n  return (tails_match)\n}\n\ncoin_simulation &lt;- function(num_coins, num_iterations) {\n  results &lt;- map_dbl(c(1:num_iterations), ~flip_coins(num_coins = num_coins)) \n  probability_same_tails &lt;- mean(results)\n  return(probability_same_tails)\n}\n\ncoin_simulation(50, 100000)\n\n[1] 0.07941\n\n\nAccording to the simulation, I could expect to get the right answer about 8% of the time!\nI’m honestly a little disappointed by this result (I was hoping the real proportion would be higher), but I’m glad I finally have my answer. Out of curiosity, I have also simulated how the results would change with varying pile sizes. A plot with the results of that simulation can be seen below.\n\nlibrary(ggplot2)\n\nnum_coins &lt;- (1:100)\n\nresults_100 &lt;- map_dbl(num_coins, coin_simulation, num_iterations = 5000)\nresults_df &lt;- data.frame(num_coins = num_coins, probability = results_100)\n\n\nggplot(results_df, aes(x = num_coins, y = probability)) +\n  geom_line(color = \"blue\") +\n  geom_point(color = \"red\") +\n  labs(\n    title = \"Probability of Equal Tails in Two Piles\",\n    x = \"Pile Size\",\n    y = \"Probability\"\n  ) +\n  theme_minimal()\n\n\n\n\n\n\n\n\nInsights\nThis plot depicts the correlation between the number of coins in each pile, and the probability that the flips will result in the same number of tails. As you can see, if you wanted to maximize your odds of success, the smartest thing to do would be to minimize the pile size. In the interview question I heard about, the pile size was 50, and we can see in the graph that this corresponds to a probability score of about 0.8 (the same as before).\nI think these results have applications outside of a random Apple interview question like this one because they speak to the best way to maximize repeated success in a probabilistic setting. These results speak to the fact that reducing the number of potential outcomes reduces the probability of getting different outcomes. In this case, you can’t reduce the number of outcomes by changing the number of sides on a coin, but you can reduce the number of outcomes by reducing the number of times you perform the experiment. This is also true in general. Imagine if you were to perform the same simulation except with 100 six-sided-dice instead of 100 two-sided coins. The results would follow the same pattern."
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "About this site\n\n1 + 1\n\n[1] 2"
  },
  {
    "objectID": "TidyTuesday2.html",
    "href": "TidyTuesday2.html",
    "title": "Cheese!",
    "section": "",
    "text": "This is an analysis of Cheese data sourced from TidyTuesday’s June 4th, 2024 data release. The plot compares the fat and calcium contents different types of cheese. To download the data, visit this github repo. This dataset was compiled using data from Cheese.com and its creation was inspired by the polite package."
  },
  {
    "objectID": "TidyTuesday1.html",
    "href": "TidyTuesday1.html",
    "title": "Water Insecurity",
    "section": "",
    "text": "This is an analysis of Water Insecurity data sourced from TidyTuesday’s January 28th, 2025 data release. The plot depicts the water insecurity of US Counties in 2022. To download the data, visit this github repo. This dataset was compiled by Niha Pereira using the tidycensus package for R and this blog walkthrough. For more information on how to access census data from sources such as the U.S. Census Bureau, the Decennial Census, the American Community Survey (ACS), and the Household Pulse Survey, see the walkthrough."
  },
  {
    "objectID": "Project2.html",
    "href": "Project2.html",
    "title": "The Office!",
    "section": "",
    "text": "df &lt;- read.csv(\"the-office_lines.csv\", fileEncoding = \"UTF-8\")\nlibrary(stringr)\nlibrary(dplyr)\nlibrary(tidyr)\nlibrary(ggplot2)\n\nQuestion 1:\nWhat episode of The Office talks about downsizing the most?\n\ndownsizing &lt;- df |&gt;\n  group_by(Season, Episode_Number) |&gt;\n  filter(str_detect(Line, regex(\"\\\\bdownsizing\\\\b\", ignore_case = TRUE))) |&gt;\n  mutate(count_of_downsizing_lines = n()) |&gt;\n  mutate(SeasonEp = paste0(\"S:\", Season, \" E:\", Episode_Number)) |&gt;\n  select(SeasonEp, count_of_downsizing_lines) |&gt;\n  distinct()\ndownsizing\n\n# A tibble: 5 × 4\n# Groups:   Season, Episode_Number [5]\n  Season Episode_Number SeasonEp count_of_downsizing_lines\n   &lt;int&gt;          &lt;int&gt; &lt;chr&gt;                        &lt;int&gt;\n1      1              1 S:1 E:1                         10\n2      1              4 S:1 E:4                         14\n3      1              5 S:1 E:5                          1\n4      2              6 S:2 E:6                          1\n5      4              1 S:4 E:1                          1\n\n\nWhen I think of downsizing in The Office, I tend to think of the very first episode (10) so I am surprised to see that, in reality, the word appears in more lines (14) in the fourth episode.\nQuestion 2:\nOf the characters that appear in the first episode, what is the average number of words per line?\n\nEp1Characters &lt;- df |&gt;\n  filter(Season == 1, Episode_Number == 1) |&gt;\n  distinct(Character)\n\nAvgWordsSpoken &lt;- df |&gt;\n  filter(Character %in% Ep1Characters$Character) |&gt;\n  group_by(Character) |&gt;\n  mutate(Count = str_count(Line, \"\\\\b\\\\w+\\\\b\")) |&gt;\n  mutate(NumLines = n()) |&gt;\n  summarise(AvgCount = mean(Count), NumLines = NumLines) |&gt;\n  arrange(desc(AvgCount)) |&gt;\n  distinct()\n\nAvgWordsSpoken\n\n# A tibble: 16 × 3\n# Groups:   Character [16]\n   Character               AvgCount NumLines\n   &lt;chr&gt;                      &lt;dbl&gt;    &lt;int&gt;\n 1 Documentary Crew Member    19           3\n 2 Todd Packer                17.6        74\n 3 Michael                    15.3     11806\n 4 Dwight                     12.6      7393\n 5 Ryan                       11.0      1324\n 6 Jan                        10.9       919\n 7 Jim                        10.4      6666\n 8 Pam                        10.0      5264\n 9 Angela                      9.90     1677\n10 Oscar                       9.87     1464\n11 Stanley                     9.68      750\n12 Michel                      9.6         5\n13 Roy                         9.56      255\n14 Kevin                       9.44     1678\n15 Phyllis                     9.00     1054\n16 Man                         8.86       44\n\n\nThese results display that there is a fairly wide range of average sentence lengths. There also seems to be a correlation between speaking often and speaking a lot. There are, of course some exceptions to this rule, but it is particularly true for Michael Scott.\nQuestion 3:\nWho has the longest ’um’s?\n\nUmTracker &lt;- df |&gt;\n  group_by(Character) |&gt;\n  mutate(umInstances = str_extract(Line, regex(\"\\\\bum+\\\\b\", ignore_case = TRUE))) |&gt;\n  filter(!is.na(umInstances)) |&gt;\n  mutate(umLength = nchar(umInstances)) |&gt;\n  summarise(maxUmLength = max(umLength), maxUmLine = Line[which.max(umLength)], .groups = \"drop\") |&gt;\n  select(Character, maxUmLength, maxUmLine) |&gt;\n  arrange(desc(maxUmLength))\n\nUmTracker\n\n# A tibble: 73 × 3\n   Character maxUmLength maxUmLine                                              \n   &lt;chr&gt;           &lt;int&gt; &lt;chr&gt;                                                  \n 1 Pam                 5 \" Ummmm…   \"                                           \n 2 Darryl              4 \" [freezes] Ummm… [a moment later] Alright. Obviously …\n 3 David               4 \"   Ummm… okay, here’s the thing though.  The plan doe…\n 4 Donna               4 \" Ummm, no.\"                                           \n 5 Jim                 4 \" Ummm… no idea.\"                                      \n 6 Kelly               4 \" Ummm, like a week ago, we got really wasted and it j…\n 7 Michael             4 \" Ummm-hmmm…\"                                          \n 8 Oscar               4 \" Ummm… \"                                              \n 9 Andy                3 \" Umm, on the contrary. \"                              \n10 Angela              3 \" I have a very important announcement to make… about……\n# ℹ 63 more rows\n\n\nPam has the longest um! In Season 2, Episode 9, she has a line in which all she says is “Ummmm…”. It makes sense for Pam’s character to have the longest ‘um’ considering how reserved she is. This reservedness reveals itself in the previous plot in that she has a relatively small average sentence length relative to the number of lines she speaks. In fact, of all of the characters that have more than 2,000 lines, she is the only one with an average length less than 10. Being unsure of herself, it makes sense that Pam would be the only character to have this long of a hesitation written into the script.\nQuestion 4:\nWhat words come before exclamation marks and question marks?\n\nWowWords &lt;- df |&gt;\n  mutate(instance = str_to_lower(str_extract(Line, \"\\\\b\\\\w+(?=!)\"))) |&gt;\n  filter(!is.na(instance)) |&gt;\n  group_by(instance) |&gt;\n  summarize(count = n(), .groups = \"drop\") |&gt;\n  arrange(desc(count))\n\nQuestionWords &lt;- df |&gt;\n  mutate(instance = str_to_lower(str_extract(Line, \"\\\\b\\\\w+(?=\\\\?)\"))) |&gt;\n  filter(!is.na(instance)) |&gt;\n  group_by(instance) |&gt;\n  summarize(count = n(), .groups = \"drop\") |&gt;\n  arrange(desc(count))\n\nPlot 1:\n\nTenWowWords &lt;- WowWords |&gt;\n  head(10)\n  \n\nggplot(TenWowWords, aes(x = instance, y = count)) +\n  geom_bar(stat = \"identity\", fill = \"dodgerblue\") +\n  coord_flip() +\n  labs(title = \"10 Most Frequent Words Before '!'\", x = \"Words\", y = \"Count\") +\n  theme_minimal()\n\n\n\n\n\n\n\n\nThis plot displays the 10 words that most often appear immediately before an exclamation mark. My favorite aspect of this plot is that for most of the words, you can imagine who said them and how. For example, just as the viewer may have guessed, Dwight said ‘Michael!’ 29 times over the course of the show, more than twice as much as any other character. Similarly, Michael ended his sentences with ‘no!’ and ‘god!’ much more often than any other character. It is also important to note that these words differ significantly from the list of 10 most used words in general.\nPlot 2:\n\nTenQuestionWords &lt;- QuestionWords |&gt;\n  head(10)\n  \n\nggplot(TenQuestionWords, aes(x = instance, y = count)) +\n  geom_bar(stat = \"identity\", fill = \"dodgerblue\") +\n  coord_flip() +\n  labs(title = \"10 Most Frequent Words Before '?'\", x = \"Words\", y = \"Count\") +\n  theme_minimal()\n\n\n\n\n\n\n\n\nThis plot displays the 10 words that most often appear immediately before a question mark. Unlike the previous plot, it is difficult to imagine who typically said each word and how. However, it is still quite interesting, though not necessarily surprisingly, that there is one word, “what”, that precedes a question mark significantly more often than any other. It is also important to note that these words differ significantly from the list of 10 most used words in general.\nPlot 3:\n\nWowWords &lt;- df |&gt;\n  mutate(instance = str_to_lower(str_extract(Line, \"\\\\b\\\\w+(?=!)\"))) |&gt;\n  filter(!is.na(instance)) |&gt;\n  group_by(instance) |&gt;\n  summarize(count_wow = n(), .groups = \"drop\")\n\nQuestionWords &lt;- df |&gt;\n  mutate(instance = str_to_lower(str_extract(Line, \"\\\\b\\\\w+(?=\\\\?)\"))) |&gt;\n  filter(!is.na(instance)) |&gt;\n  group_by(instance) |&gt;\n  summarize(count_question = n(), .groups = \"drop\")\n\ncommonWords &lt;- full_join(WowWords, QuestionWords, by = \"instance\") |&gt;\n  mutate(\n    count_wow = ifelse(is.na(count_wow), 0, count_wow),         \n    count_question = ifelse(is.na(count_question), 0, count_question), \n    total_wow = sum(count_wow),                                 \n    total_question = sum(count_question),                       \n    prop_wow = count_wow / total_wow,                \n    prop_question = count_question / total_question   \n  ) |&gt;\n  arrange(desc(prop_wow + prop_question)) |&gt;\n  head(10) \n\nggplot(commonWords, aes(x = prop_wow, y = prop_question, label = instance)) +\n  geom_point(stat = \"identity\", color = \"#1f77b4\", size = 3) +\n  geom_text(vjust = -0.25, hjust = -0.5, size = 4, color = \"black\") + \n  labs(\n    title = \"Relative Frequency of Words Before '?' and '!'\",\n    subtitle = \"Top 10 most frequent words\",\n    x = \"Proportion Before '!'\",\n    y = \"Proportion Before '?'\"\n  ) +\n  theme_minimal(base_size = 15) + \n  theme(\n    legend.position = \"none\",\n    axis.title = element_text(size = 14),\n    axis.text = element_text(size = 12),\n    plot.title = element_text(size = 18, face = \"bold\"),\n    plot.subtitle = element_text(size = 14, face = \"italic\")\n  )\n\n\n\n\n\n\n\n\nThis graph plots the relative frequencies of the 10 most common words used before exclamation marks and question marks. Some words, such as ‘you’ and ‘it’ precede each sign with similar frequencies. Most others, including ‘what’, ‘no’, ‘oh’, ‘hey’, and ‘yes’ differ significantly in their proportions. Among these, ‘what’ is the biggest outlier because it precedes nearly 1% of all question marks in the show! No other word appears before either of the signs with even half that frequency.\nData Source\nThe dataset used in this analysis is “The Office Lines” dataset on Kaggle. This dataset was currated by scraping text from The Office Quotes. It contains transcripts from all episodes of The Office (U.S.), including character dialogue, season, and episode numbers for every line."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Jack Susank",
    "section": "",
    "text": "Hi, I’m Jack Susank and welcome to my website! I’m a 3rd year computer science student at Pomona College with a passion for problem-solving, AI, and technology. Recently, I’ve been looking into Natural Language Processing (NLP), 3D Rendering/Printing, Data Visualization, and App Building. When I’m not working, studying, or learning about these things, I enjoy hiking, cooking, playing football, and spending time with my friends and family.\nThanks for visiting my site! Feel free to check out my projects and get in touch!"
  }
]