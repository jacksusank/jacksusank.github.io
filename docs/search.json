[
  {
    "objectID": "TidyTuesday1.html",
    "href": "TidyTuesday1.html",
    "title": "Water Insecurity",
    "section": "",
    "text": "This is an analysis of Water Insecurity data sourced from TidyTuesday’s January 28th, 2025 data release. The plot depicts the water insecurity of US Counties in 2022. To download the data, visit this github repo. This dataset was compiled by Niha Pereira using the tidycensus package for R and this blog walkthrough. For more information on how to access census data from sources such as the U.S. Census Bureau, the Decennial Census, the American Community Survey (ACS), and the Household Pulse Survey, see the walkthrough."
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "About this site\n\n1 + 1\n\n[1] 2"
  },
  {
    "objectID": "Project2.html",
    "href": "Project2.html",
    "title": "The Office!",
    "section": "",
    "text": "df &lt;- read.csv(\"the-office_lines.csv\", fileEncoding = \"UTF-8\")\nlibrary(stringr)\nlibrary(dplyr)\nlibrary(tidyr)\nlibrary(ggplot2)\n\nQuestion 1:\nWhat episode of The Office talks about downsizing the most?\n\ndownsizing &lt;- df |&gt;\n  group_by(Season, Episode_Number) |&gt;\n  filter(str_detect(Line, regex(\"\\\\bdownsizing\\\\b\", ignore_case = TRUE))) |&gt;\n  mutate(count_of_downsizing_lines = n()) |&gt;\n  mutate(SeasonEp = paste0(\"S:\", Season, \" E:\", Episode_Number)) |&gt;\n  select(SeasonEp, count_of_downsizing_lines) |&gt;\n  distinct()\ndownsizing\n\n# A tibble: 5 × 4\n# Groups:   Season, Episode_Number [5]\n  Season Episode_Number SeasonEp count_of_downsizing_lines\n   &lt;int&gt;          &lt;int&gt; &lt;chr&gt;                        &lt;int&gt;\n1      1              1 S:1 E:1                         10\n2      1              4 S:1 E:4                         14\n3      1              5 S:1 E:5                          1\n4      2              6 S:2 E:6                          1\n5      4              1 S:4 E:1                          1\n\n\nWhen I think of downsizing in The Office, I tend to think of the very first episode (10) so I am surprised to see that, in reality, the word appears in more lines (14) in the fourth episode.\nQuestion 2:\nOf the characters that appear in the first episode, what is the average number of words per line?\n\nEp1Characters &lt;- df |&gt;\n  filter(Season == 1, Episode_Number == 1) |&gt;\n  distinct(Character)\n\nAvgWordsSpoken &lt;- df |&gt;\n  filter(Character %in% Ep1Characters$Character) |&gt;\n  group_by(Character) |&gt;\n  mutate(Count = str_count(Line, \"\\\\b\\\\w+\\\\b\")) |&gt;\n  mutate(NumLines = n()) |&gt;\n  summarise(AvgCount = mean(Count), NumLines = NumLines) |&gt;\n  arrange(desc(AvgCount)) |&gt;\n  distinct()\n\nAvgWordsSpoken\n\n# A tibble: 16 × 3\n# Groups:   Character [16]\n   Character               AvgCount NumLines\n   &lt;chr&gt;                      &lt;dbl&gt;    &lt;int&gt;\n 1 Documentary Crew Member    19           3\n 2 Todd Packer                17.6        74\n 3 Michael                    15.3     11806\n 4 Dwight                     12.6      7393\n 5 Ryan                       11.0      1324\n 6 Jan                        10.9       919\n 7 Jim                        10.4      6666\n 8 Pam                        10.0      5264\n 9 Angela                      9.90     1677\n10 Oscar                       9.87     1464\n11 Stanley                     9.68      750\n12 Michel                      9.6         5\n13 Roy                         9.56      255\n14 Kevin                       9.44     1678\n15 Phyllis                     9.00     1054\n16 Man                         8.86       44\n\n\nThese results display that there is a fairly wide range of average sentence lengths. There also seems to be a correlation between speaking often and speaking a lot. There are, of course some exceptions to this rule, but it is particularly true for Michael Scott.\nQuestion 3:\nWho has the longest ’um’s?\n\nUmTracker &lt;- df |&gt;\n  group_by(Character) |&gt;\n  mutate(umInstances = str_extract(Line, regex(\"\\\\bum+\\\\b\", ignore_case = TRUE))) |&gt;\n  filter(!is.na(umInstances)) |&gt;\n  mutate(umLength = nchar(umInstances)) |&gt;\n  summarise(maxUmLength = max(umLength), maxUmLine = Line[which.max(umLength)], .groups = \"drop\") |&gt;\n  select(Character, maxUmLength, maxUmLine) |&gt;\n  arrange(desc(maxUmLength))\n\nUmTracker\n\n# A tibble: 73 × 3\n   Character maxUmLength maxUmLine                                              \n   &lt;chr&gt;           &lt;int&gt; &lt;chr&gt;                                                  \n 1 Pam                 5 \" Ummmm…   \"                                           \n 2 Darryl              4 \" [freezes] Ummm… [a moment later] Alright. Obviously …\n 3 David               4 \"   Ummm… okay, here’s the thing though.  The plan doe…\n 4 Donna               4 \" Ummm, no.\"                                           \n 5 Jim                 4 \" Ummm… no idea.\"                                      \n 6 Kelly               4 \" Ummm, like a week ago, we got really wasted and it j…\n 7 Michael             4 \" Ummm-hmmm…\"                                          \n 8 Oscar               4 \" Ummm… \"                                              \n 9 Andy                3 \" Umm, on the contrary. \"                              \n10 Angela              3 \" I have a very important announcement to make… about……\n# ℹ 63 more rows\n\n\nPam has the longest um! In Season 2, Episode 9, she has a line in which all she says is “Ummmm…”. It makes sense for Pam’s character to have the longest ‘um’ considering how reserved she is. This reservedness reveals itself in the previous plot in that she has a relatively small average sentence length relative to the number of lines she speaks. In fact, of all of the characters that have more than 2,000 lines, she is the only one with an average length less than 10. Being unsure of herself, it makes sense that Pam would be the only character to have this long of a hesitation written into the script.\nQuestion 4:\nWhat words come before exclamation marks and question marks?\n\nWowWords &lt;- df |&gt;\n  mutate(instance = str_to_lower(str_extract(Line, \"\\\\b\\\\w+(?=!)\"))) |&gt;\n  filter(!is.na(instance)) |&gt;\n  group_by(instance) |&gt;\n  summarize(count = n(), .groups = \"drop\") |&gt;\n  arrange(desc(count))\n\nQuestionWords &lt;- df |&gt;\n  mutate(instance = str_to_lower(str_extract(Line, \"\\\\b\\\\w+(?=\\\\?)\"))) |&gt;\n  filter(!is.na(instance)) |&gt;\n  group_by(instance) |&gt;\n  summarize(count = n(), .groups = \"drop\") |&gt;\n  arrange(desc(count))\n\nPlot 1:\n\nTenWowWords &lt;- WowWords |&gt;\n  head(10)\n  \n\nggplot(TenWowWords, aes(x = instance, y = count)) +\n  geom_bar(stat = \"identity\", fill = \"dodgerblue\") +\n  coord_flip() +\n  labs(title = \"10 Most Frequent Words Before '!'\", x = \"Words\", y = \"Count\") +\n  theme_minimal()\n\n\n\n\n\n\n\n\nThis plot displays the 10 words that most often appear immediately before an exclamation mark. My favorite aspect of this plot is that for most of the words, you can imagine who said them and how. For example, just as the viewer may have guessed, Dwight said ‘Michael!’ 29 times over the course of the show, more than twice as much as any other character. Similarly, Michael ended his sentences with ‘no!’ and ‘god!’ much more often than any other character. It is also important to note that these words differ significantly from the list of 10 most used words in general.\nPlot 2:\n\nTenQuestionWords &lt;- QuestionWords |&gt;\n  head(10)\n  \n\nggplot(TenQuestionWords, aes(x = instance, y = count)) +\n  geom_bar(stat = \"identity\", fill = \"dodgerblue\") +\n  coord_flip() +\n  labs(title = \"10 Most Frequent Words Before '?'\", x = \"Words\", y = \"Count\") +\n  theme_minimal()\n\n\n\n\n\n\n\n\nThis plot displays the 10 words that most often appear immediately before a question mark. Unlike the previous plot, it is difficult to imagine who typically said each word and how. However, it is still quite interesting, though not necessarily surprisingly, that there is one word, “what”, that precedes a question mark significantly more often than any other. It is also important to note that these words differ significantly from the list of 10 most used words in general.\nPlot 3:\n\nWowWords &lt;- df |&gt;\n  mutate(instance = str_to_lower(str_extract(Line, \"\\\\b\\\\w+(?=!)\"))) |&gt;\n  filter(!is.na(instance)) |&gt;\n  group_by(instance) |&gt;\n  summarize(count_wow = n(), .groups = \"drop\")\n\nQuestionWords &lt;- df |&gt;\n  mutate(instance = str_to_lower(str_extract(Line, \"\\\\b\\\\w+(?=\\\\?)\"))) |&gt;\n  filter(!is.na(instance)) |&gt;\n  group_by(instance) |&gt;\n  summarize(count_question = n(), .groups = \"drop\")\n\ncommonWords &lt;- full_join(WowWords, QuestionWords, by = \"instance\") |&gt;\n  mutate(\n    count_wow = ifelse(is.na(count_wow), 0, count_wow),         \n    count_question = ifelse(is.na(count_question), 0, count_question), \n    total_wow = sum(count_wow),                                 \n    total_question = sum(count_question),                       \n    prop_wow = count_wow / total_wow,                \n    prop_question = count_question / total_question   \n  ) |&gt;\n  arrange(desc(prop_wow + prop_question)) |&gt;\n  head(10) \n\nggplot(commonWords, aes(x = prop_wow, y = prop_question, label = instance)) +\n  geom_point(stat = \"identity\", color = \"#1f77b4\", size = 3) +\n  geom_text(vjust = -0.25, hjust = -0.5, size = 4, color = \"black\") + \n  labs(\n    title = \"Relative Frequency of Words Before '?' and '!'\",\n    subtitle = \"Top 10 most frequent words\",\n    x = \"Proportion Before '!'\",\n    y = \"Proportion Before '?'\"\n  ) +\n  theme_minimal(base_size = 15) + \n  theme(\n    legend.position = \"none\",\n    axis.title = element_text(size = 14),\n    axis.text = element_text(size = 12),\n    plot.title = element_text(size = 18, face = \"bold\"),\n    plot.subtitle = element_text(size = 14, face = \"italic\")\n  )\n\n\n\n\n\n\n\n\nThis graph plots the relative frequencies of the 10 most common words used before exclamation marks and question marks. Some words, such as ‘you’ and ‘it’ precede each sign with similar frequencies. Most others, including ‘what’, ‘no’, ‘oh’, ‘hey’, and ‘yes’ differ significantly in their proportions. Among these, ‘what’ is the biggest outlier because it precedes nearly 1% of all question marks in the show! No other word appears before either of the signs with even half that frequency.\nData Source\nThe dataset used in this analysis is “The Office Lines” dataset on Kaggle. This dataset was currated by scraping text from The Office Quotes. It contains transcripts from all episodes of The Office (U.S.), including character dialogue, season, and episode numbers for every line."
  },
  {
    "objectID": "Project4.html",
    "href": "Project4.html",
    "title": "Data Ethics",
    "section": "",
    "text": "There exists a growing problem in the field of data science which pertains to how data is sourced. Useful data oftentimes comes from individuals, but there are a whole host of problems that could arise when this data can be linked to the person it describes. Medical data for example, can contain very personal details about a person’s life. In the wrong hands, these details could be used to profit off of the individual and potentially at their expense. The process of eliminating the possibility of the data being reconnected with the person it describes, is called de-identification, and it may appear that removing names from a dataset is enough to guarantee this, but this is not the case! It turns out that there are several other details that could be used as identifiers and “the HIPAA Privacy Rule designates 18” of them (NashBio). Unfortunately, removing this identifiers is oftentimes not enough to de-identify the data. This is because other quasi-identifiers can be used in conjunction with one another to piece together a person’s (or community’s) identity. The “concept of combing datasets to fill in the blanks is known as ‘mosaicking’” (Forbes) and as an example, we can look at the famous New York City taxi database and the research performed by grad student Anthony Tockar. When the taxi dataset was released in 2013, it was ‘anonymized’ by performing a mathematical operation (called MD5 hashing) on the license and medallion numbers of each cab driver. This would have sufficiently scrambled these identifiers if the size of the input space wasn’t so small.Unfortunately, it was, and because the researchers did not do any salting or keying, people like Tockar (Forbes) and Vijay Pandurangan (ArsTechnica) were able to reverse engineer the original values. Combining this knowledge with the vast numbers of pictures of celebrities in cabs published by the paparazzi everyday, they were then able to match the pictures with their corresponding datatable entry. The cabs and their plate and medallion numbers are clearly depicted in many of these pictures, so by combining this information with the picture’s metadata (like time and location), Tockar was easily able to identify the celebrities, their destinations, as well as how much they paid and tipped for each ride. Examples like this reveal how difficult it can be to de-identify data, but luckily other solutions exist. Data managers are able to perform a variety of data manipulation techniques including clustering data into groups, providing row averages instead of individual values, and even producing ‘synthetic data’ that randomizes the data while maintaining the same distribution within each dimension and capturing inter-dimensional correlations. Approaches such as these allow researchers to draw meaningful conclusions while maintaining a strict de-identification status.\nResponse Questions (New York City Taxi Dataset):\nYes, the dataset was made available to the public by New York City officials in 2013 and it contained detailed information about over 173 million taxi rides (ArsTechnica). The observational units were cab rides and each one included information on trip routes, trip times, and ‘anonymized’ identifiers for drivers and their vehicles. Although the identifying information was hashed using a method called MD5, the full 20GB dataset was downloadable by anyone and was susceptible to de-anonymization. This level of public accessibility, combined with the flawed de-identification method, meant that sensitive details about drivers and their work patterns were effectively released to the public.\nUnfortunately, despite the intentions of the researchers, the data are identifiable. Each of the table entries can be successfully de-anonymized because although the dataset managers attempted to anonymize it using one-way MD5 hashes on the medallion and license numbers, the structure of those numbers was relatively restricted and therefore susceptible to cryptographic de-anonymization given some time (ArsTechnica). This small input space made it trivial for someone like software developer Vijay Pandurangan to generate all possible hashes and match them against the released dataset. Consequently, anonymity was not guaranteed in any sense. The poor choice of MD5 hashing without keying or salting raises significant ethical concerns and rendered the data highly vulnerable to re-identification. Because the dataset enabled the public to track specific drivers’ behaviors, work habits, and potentially even home locations, its release constitutes a serious breach of privacy and being released in 2013, it is not old enough to warrant exemptions for these kind of mistakes (Forbes).\nYes, the data were absolutely used in unintended ways. While the original intent behind releasing the dataset may have been to support transparency within the government and promote research or innovation around public transportation, the poor anonymization opened the door for other intentions. Not only were the cab drivers travel and earnings patterns exposed, but if you ever had a way of knowing the plate number/medallion and the time and location of specific person’s cab ride, then you also had access to information like their destination and how much they tipped (Forbes)! Surely, this was not the intention of the city officials, but regardless, it is on them to eliminate any possibility of things like this happening. This specific repurping of the data extended far beyond the intended use and exposes how data, even when altered or partially scrubbed to the extent that it seems unidentifiable, can still be misused (NashBio).\nWhile the intentions behind the release of the data may have been benevolent, and in general, we must recognize that a move towards transparency within the government is positive, the data did not end up effectively serving to improve life for users, drivers, or communities. Particularly due to its lack of privacy, the ethical concerns generated by the flawed anonymization method overshadow any possible benefits that the data might have offered. Instead of boosting transparency and promoting public transportation (such as by fostering efficiency gains) within communities, the release exposed drivers to potential scrutiny, surveillance, and exploitation, which undoubtedly could do more harm than good (ArsTechnica)."
  },
  {
    "objectID": "Project4.html#works-cited",
    "href": "Project4.html#works-cited",
    "title": "Data Ethics",
    "section": "Works Cited",
    "text": "Works Cited\n“De-Identification: Balancing Privacy and Utility in Healthcare Data.” Nashville Biosciences, 24 Jan. 2025, nashbio.com/blog/healthcare-data/de-identification-balancing-privacy-and-utility-in-healthcare-data/.\nGoodin, Dan. “Poorly Anonymized Logs Reveal NYC Cab Drivers’ Detailed Whereabouts.” Ars Technica, 23 June 2014, https://arstechnica.com/tech-policy/2014/06/poorly-anonymized-logs-reveal-nyc-cab-drivers-detailed-whereabouts/.\nLeetaru, Kalev. “The Big Data Era of Mosaicked Deidentification: Can We Anonymize Data Anymore?” Forbes, 24 Aug. 2016, https://www.forbes.com/sites/kalevleetaru/2016/08/24/the-big-data-era-of-mosaicked-deidentification-can-we-anonymize-data-anymore/."
  },
  {
    "objectID": "TidyTuesday2.html",
    "href": "TidyTuesday2.html",
    "title": "Cheese!",
    "section": "",
    "text": "This is an analysis of Cheese data sourced from TidyTuesday’s June 4th, 2024 data release. The plot compares the fat and calcium contents different types of cheese. To download the data, visit this github repo. This dataset was compiled using data from Cheese.com and its creation was inspired by the polite package."
  },
  {
    "objectID": "Project5.html",
    "href": "Project5.html",
    "title": "Police Stops",
    "section": "",
    "text": "con_traffic &lt;- DBI::dbConnect(\n  RMariaDB::MariaDB(),\n  dbname = \"traffic\",\n  host = Sys.getenv(\"TRAFFIC_HOST\"),\n  user = Sys.getenv(\"TRAFFIC_USER\"),\n  password = Sys.getenv(\"TRAFFIC_PWD\")\n)\n\nHere are the names of all the tables that we have at our disposal.\n\nSHOW TABLES;\n\n\nDisplaying records 1 - 10\n\n\nTables_in_traffic\n\n\n\n\nar_little_rock_2020_04_01\n\n\naz_gilbert_2020_04_01\n\n\naz_mesa_2023_01_26\n\n\naz_statewide_2020_04_01\n\n\nca_anaheim_2020_04_01\n\n\nca_bakersfield_2020_04_01\n\n\nca_long_beach_2020_04_01\n\n\nca_los_angeles_2020_04_01\n\n\nca_oakland_2020_04_01\n\n\nca_san_bernardino_2020_04_01\n\n\n\n\n\nHere is an example of what the data looks like for a given state.\n\nSELECT * FROM ca_statewide_2023_01_26 LIMIT 10;\n\n\nDisplaying records 1 - 10\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nraw_row_number\ndate\ncounty_name\ndistrict\nsubject_race\nsubject_sex\ndepartment_name\ntype\nviolation\narrest_made\ncitation_issued\nwarning_issued\noutcome\ncontraband_found\nfrisk_performed\nsearch_conducted\nsearch_person\nsearch_basis\nreason_for_stop\nraw_race\nraw_search_basis\nraw_location_code\n\n\n\n\n1\n2009-07-01\nStanislaus\nModesto\nother\nmale\nCalifornia Highway Patrol\nvehicular\nMotorist / Public Service\nNA\nNA\nNA\nNA\nNA\nNA\n0\n0\nNA\nMotorist / Public Service\nOther\nVehicle Inventory\n465\n\n\n2\n2009-07-01\nStanislaus\nModesto\nhispanic\nfemale\nCalifornia Highway Patrol\nvehicular\nMoving Violation (VC)\n0\n0\n0\nsummons\nNA\nNA\n0\n0\nNA\nMoving Violation (VC)\nHispanic\nProbable Cause (positive)\n465\n\n\n3\n2009-07-01\nStanislaus\nModesto\nhispanic\nfemale\nCalifornia Highway Patrol\nvehicular\nMoving Violation (VC)\n0\n0\n0\nsummons\nNA\nNA\n1\nNA\nother\nMoving Violation (VC)\nHispanic\nProbable Cause (positive)\n465\n\n\n4\n2009-07-01\nStanislaus\nModesto\nwhite\nfemale\nCalifornia Highway Patrol\nvehicular\nMoving Violation (VC)\n0\n0\n0\nsummons\nNA\nNA\n0\n0\nNA\nMoving Violation (VC)\nWhite\nProbable Cause (positive)\n465\n\n\n5\n2009-07-01\nStanislaus\nModesto\nhispanic\nmale\nCalifornia Highway Patrol\nvehicular\nMoving Violation (VC)\n0\n0\n0\nsummons\nNA\nNA\n1\nNA\nother\nMoving Violation (VC)\nHispanic\nProbable Cause (positive)\n465\n\n\n6\n2009-07-01\nStanislaus\nModesto\nhispanic\nmale\nCalifornia Highway Patrol\nvehicular\nMoving Violation (VC)\n0\n0\n0\nsummons\nNA\nNA\n0\n0\nNA\nMoving Violation (VC)\nHispanic\nProbable Cause (positive)\n465\n\n\n7\n2009-07-01\nStanislaus\nModesto\nhispanic\nfemale\nCalifornia Highway Patrol\nvehicular\nMoving Violation (VC)\n0\n0\n0\nsummons\nNA\nNA\n0\n0\nNA\nMoving Violation (VC)\nHispanic\nProbable Cause (positive)\n465\n\n\n8\n2009-07-01\nStanislaus\nModesto\nother\nfemale\nCalifornia Highway Patrol\nvehicular\nMoving Violation (VC)\n0\n0\n0\nsummons\nNA\nNA\n0\n0\nNA\nMoving Violation (VC)\nOther\nProbable Cause (positive)\n465\n\n\n9\n2009-07-01\nStanislaus\nModesto\nhispanic\nmale\nCalifornia Highway Patrol\nvehicular\nMoving Violation (VC)\n0\n0\n0\nsummons\nNA\nNA\n0\n0\nNA\nMoving Violation (VC)\nHispanic\nProbable Cause (positive)\n465\n\n\n10\n2009-07-01\nStanislaus\nModesto\nwhite\nfemale\nCalifornia Highway Patrol\nvehicular\nMechanical or Nonmoving Violation (VC)\n0\n0\n1\nwarning\nNA\nNA\n0\n0\nNA\nMechanical or Nonmoving Violation (VC)\nWhite\nProbable Cause (negative)\n465\n\n\n\n\n\nThis week, I want to answer the question of which sex has the most police interactions and which gets off with a warning most often? I will be analyzing the data from police encounters in three western states, California, Colorado, and Arizona, and then I will present the results for each of them. I wonder if the results will differ based on the state, and I wonder if there will be a significant difference between the number of warnings given to males and females.\nI will begin by extracting the relevant data from from each state’s statewide data table by joining tables containing the total number of encounters for each sex with tables containing the total number of warnings for each sex.\nArizona:\n\nSELECT \n  total.subject_sex,\n  total.total_stops,\n  warnings.warning_stops,\n  'Arizona' AS state\nFROM\n  (SELECT subject_sex, COUNT(*) AS total_stops\n   FROM az_statewide_2020_04_01\n   GROUP BY subject_sex) AS total\nLEFT JOIN\n  (SELECT subject_sex, COUNT(*) AS warning_stops\n   FROM az_statewide_2020_04_01\n   WHERE outcome = 'warning'\n   GROUP BY subject_sex) AS warnings\nON total.subject_sex = warnings.subject_sex\nORDER BY total.total_stops DESC;\n\nColorado:\n\nSELECT \n  total.subject_sex,\n  total.total_stops,\n  warnings.warning_stops,\n  'Colorado' AS state\nFROM\n  (SELECT subject_sex, COUNT(*) AS total_stops\n   FROM co_statewide_2020_04_01\n   GROUP BY subject_sex) AS total\nLEFT JOIN\n  (SELECT subject_sex, COUNT(*) AS warning_stops\n   FROM co_statewide_2020_04_01\n   WHERE outcome = 'warning'\n   GROUP BY subject_sex) AS warnings\nON total.subject_sex = warnings.subject_sex\nORDER BY total.total_stops DESC;\n\nCalifornia:\n\nSELECT \n  total.subject_sex,\n  total.total_stops,\n  warnings.warning_stops,\n  'California' AS state\nFROM\n  (SELECT subject_sex, COUNT(*) AS total_stops\n   FROM ca_statewide_2023_01_26\n   GROUP BY subject_sex) AS total\nLEFT JOIN\n  (SELECT subject_sex, COUNT(*) AS warning_stops\n   FROM ca_statewide_2023_01_26\n   WHERE outcome = 'warning'\n   GROUP BY subject_sex) AS warnings\nON total.subject_sex = warnings.subject_sex\nORDER BY total.total_stops DESC;\n\nNow that I have stored the data from each State in my environment, I will combine them into one clean table called ‘combined’.\n\nlibrary(dplyr)\n\ncombined &lt;- bind_rows(California, Colorado, Arizona)\n\ncombined &lt;- combined |&gt;\n  mutate(\n    warning_rate = warning_stops / total_stops,\n    subject_sex = recode(subject_sex, 'M' = 'Male', 'F' = 'Female')\n  )\n\ncombined\n\n  subject_sex total_stops warning_stops      state warning_rate\n1        male    22161713       3752285 California    0.1693139\n2      female     9616706       1604121 California    0.1668057\n3        &lt;NA&gt;          96          &lt;NA&gt; California           NA\n4        male     1527440        538666   Colorado    0.3526594\n5        &lt;NA&gt;      896526          &lt;NA&gt;   Colorado           NA\n6      female      688887        256601   Colorado    0.3724863\n7        male     2429540        582966    Arizona    0.2399491\n8      female     1064582        307681    Arizona    0.2890158\n9        &lt;NA&gt;        4037          &lt;NA&gt;    Arizona           NA\n\n\nFinally, I can observe the differences between the Warning Rates amongst the sexes for each state.\n\nlibrary(ggplot2)\n\ncombined |&gt;\n  filter(!is.na(subject_sex)) |&gt;\n  ggplot(aes(x = state, y = warning_rate, fill = subject_sex)) +\n    geom_col(position = \"dodge\") +\n    scale_y_continuous(labels = scales::percent) +\n    labs(\n      title = \"Warning Rate by Sex in in AZ, CA, and CO\",\n      x = \"State\",\n      y = \"Warning Rate\",\n      fill = \"Sex\"\n    ) +\n    theme_minimal()\n\n\n\n\n\n\n\n\nThis plot displays the percentage of police encounters that result in a warning for each sex, broken up by State.\nIn Arizona and Colorado, females were given a warning slightly more often than males, but in California, the proportions are essentially the same. This was somewhat surprising to me because in movies and media, it is fairly common to either make-fun or depict the apparent concept that women can appeal to their looks to get off more easily. We must analyze more data to be sure one way or the other, but the idea that women get let off with a warning more often than men do is not so readily apparent. It should also be noted that the data table from California is much larger than either of the two other states, so variance is less likely to interfere with the result.\n\nlibrary(ggplot2)\nlibrary(dplyr)\n\ncombined |&gt;\n  filter(!is.na(subject_sex)) |&gt;\n  group_by(state, subject_sex, .drop = TRUE) |&gt;\n  summarise(total_stops = sum(total_stops), .groups = \"drop\") |&gt;\n  ggplot(aes(x = state, y = total_stops, fill = subject_sex)) +\n    geom_col(position = \"dodge\") +\n    labs(\n      title = \"Total Police Encounters by Sex in AZ, CA, and CO\",\n      x = \"State\",\n      y = \"Number of Encounters\",\n      fill = \"Sex\"\n    ) +\n    theme_minimal()\n\n\n\n\n\n\n\n\nThis plot displays the total number of police stops by gender in each of the three states, Colorado, Arizona, and California, and the results are very significant! In each case, males encounter police more than twice as often as females do! Although this fact has been well documented, it has not been depicted nearly is much in our society’s media culture. Of course, the reasons for this large of a discrepancy are likely multiple, but among them could be a systemic propensity for cops to choose to pull-over or interact with males over females. Other research also suggests that the majority of this difference stems from the two facts that men drive more often than women and tend to be riskier and more aggressive.\nReferences:\nPierson, Emma, Camelia Simoiu, Jan Overgoor, Sam Corbett-Davies, Daniel Jenson, Amy Shoemaker, Vignesh Ramachandran, et al. 2020. “A Large-Scale Analysis of Racial Disparities in Police Stops Across the United States.” Nature Human Behaviour, 1–10."
  },
  {
    "objectID": "Project3.html",
    "href": "Project3.html",
    "title": "Apple Interview Question!",
    "section": "",
    "text": "Inspiration\nI once heard of a Apple interview question in which the candidate is given 100 coins (90 with heads facing up and 10 with tails facing up) and then asked to separate them into two piles of any size such that each pile has the same number of coins with the tails side facing up. Sounds simple enough, but there’s a catch… the candidate was to be blindfolded the entire time. This means they never saw the coins and have no idea which 10 coins are on tails or which 90 coins are on heads.\nThere is an algorithmic approach to this problem such that you can guarantee your answer will be true, but I was not able to come up with it. The best I could think to do is to randomly perform 100 coin flips and then separate my flipped coins into two piles of 50. I would then just have to hope that each pile has the same number of tails facing up.\nMotivating Question\nAlthough my answer is not exactly what the interviewers would have in mind, I have always been curious as to how often my answer would end up being correct merely by chance. Until now, I haven’t been able to figure out a way to answer this question without doing long mathematical calculations, and the free version of ChatGPT seems to give me a different answer each time I ask it, so I’m very excited to finally approximate the answer using this simulation.\nResults\n\nlibrary(purrr)\n\ncoin_flip &lt;- c(0, 1)\n\nflip_coins &lt;- function(pile_size) {\n  left_pile &lt;- sample(coin_flip, size = pile_size, replace = TRUE)\n  right_pile &lt;- sample(coin_flip, size = pile_size, replace = TRUE)\n  \n  tails_match &lt;- ifelse(sum(left_pile) == sum(right_pile), 1, 0)\n  return (tails_match)\n}\n\ncoin_simulation &lt;- function(pile_size, num_iterations) {\n  results &lt;- map_dbl(c(1:num_iterations), ~flip_coins(pile_size = pile_size)) \n  probability_same_tails &lt;- mean(results)\n  return(probability_same_tails)\n}\n\ncoin_simulation(50, 100000)\n\n[1] 0.08026\n\n\nAccording to the simulation, I could expect to get the right answer about 8% of the time!\nI’m honestly a little disappointed by this result (I was hoping the real proportion would be higher), but I’m glad I finally have my answer. Out of curiosity, I have also simulated how the results would change with varying pile sizes. A plot with the results of that simulation can be seen below.\n\nlibrary(ggplot2)\n\npile_size &lt;- (1:100)\n\nresults_100 &lt;- map_dbl(pile_size, coin_simulation, num_iterations = 5000)\nresults_df &lt;- data.frame(pile_size = pile_size, probability = results_100)\n\n\nggplot(results_df, aes(x = pile_size, y = probability)) +\n  geom_line(color = \"blue\") +\n  geom_point(color = \"red\") +\n  labs(\n    title = \"Probability of Equal Tails in Two Piles\",\n    x = \"Pile Size\",\n    y = \"Probability\"\n  ) +\n  theme_minimal()\n\n\n\n\n\n\n\n\nInsights\nThis plot depicts the correlation between the number of coins in each pile, and the probability that the flips will result in the same number of tails. As you can see, if you wanted to maximize your odds of success, the smartest thing to do would be to minimize the pile size. In the interview question I heard about, the pile size was 50, and we can see in the graph that this corresponds to a probability score of about 0.8 (the same as before).\nI think these results have applications outside of a random Apple interview question like this one because they speak to the best way to maximize repeated success in a probabilistic setting. These results speak to the fact that reducing the number of potential outcomes reduces the probability of getting different outcomes. In this case, you can’t reduce the number of outcomes by changing the number of sides on a coin, but you can reduce the number of outcomes by reducing the number of times you perform the experiment. This is also true in general. Imagine if you were to perform the same simulation except with 100 six-sided-dice instead of 100 two-sided coins. The results would follow the same pattern."
  }
]