[
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "About this site\n\n1 + 1\n\n[1] 2"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Jack Susank",
    "section": "",
    "text": "Hi, I’m Jack Susank and welcome to my website! I’m a 3rd year computer science student at Pomona College with a passion for problem-solving, AI, and technology. Recently, I’ve been looking into Natural Language Processing (NLP), 3D Rendering/Printing, Data Visualization, and App Building. When I’m not working, studying, or learning about these things, I enjoy hiking, cooking, playing football, and spending time with my friends and family.\nThanks for visiting my site! Feel free to check out my projects and get in touch!"
  },
  {
    "objectID": "TidyTuesday1.html",
    "href": "TidyTuesday1.html",
    "title": "Water Insecurity",
    "section": "",
    "text": "This is an analysis of Water Insecurity data sourced from TidyTuesday’s January 28th, 2025 data release. The plot depicts the water insecurity of US Counties in 2022. To download the data, visit this github repo. This dataset was compiled by Niha Pereira using the tidycensus package for R and this blog walkthrough. For more information on how to access census data from sources such as the U.S. Census Bureau, the Decennial Census, the American Community Survey (ACS), and the Household Pulse Survey, see the walkthrough."
  },
  {
    "objectID": "TidyTuesday2.html",
    "href": "TidyTuesday2.html",
    "title": "Cheese!",
    "section": "",
    "text": "This is an analysis of Cheese data sourced from TidyTuesday’s June 4th, 2024 data release. The plot compares the fat and calcium contents different types of cheese. To download the data, visit this github repo. This dataset was compiled using data from Cheese.com and its creation was inspired by the polite package."
  },
  {
    "objectID": "Project2.html",
    "href": "Project2.html",
    "title": "The Office!",
    "section": "",
    "text": "df &lt;- read.csv(\"the-office_lines.csv\", fileEncoding = \"UTF-8\")\nlibrary(stringr)\nlibrary(dplyr)\nlibrary(tidyr)\nlibrary(ggplot2)\n\nQuestion 1:\nWhat episode of The Office talks about downsizing the most?\n\ndownsizing &lt;- df |&gt;\n  group_by(Season, Episode_Number) |&gt;\n  filter(str_detect(Line, regex(\"\\\\bdownsizing\\\\b\", ignore_case = TRUE))) |&gt;\n  mutate(count_of_downsizing_lines = n()) |&gt;\n  mutate(SeasonEp = paste0(\"S:\", Season, \" E:\", Episode_Number)) |&gt;\n  select(SeasonEp, count_of_downsizing_lines) |&gt;\n  distinct()\ndownsizing\n\n# A tibble: 5 × 4\n# Groups:   Season, Episode_Number [5]\n  Season Episode_Number SeasonEp count_of_downsizing_lines\n   &lt;int&gt;          &lt;int&gt; &lt;chr&gt;                        &lt;int&gt;\n1      1              1 S:1 E:1                         10\n2      1              4 S:1 E:4                         14\n3      1              5 S:1 E:5                          1\n4      2              6 S:2 E:6                          1\n5      4              1 S:4 E:1                          1\n\n\nWhen I think of downsizing in The Office, I tend to think of the very first episode (10) so I am surprised to see that, in reality, the word appears in more lines (14) in the fourth episode.\nQuestion 2:\nOf the characters that appear in the first episode, what is the average number of words per line?\n\nEp1Characters &lt;- df |&gt;\n  filter(Season == 1, Episode_Number == 1) |&gt;\n  distinct(Character)\n\nAvgWordsSpoken &lt;- df |&gt;\n  filter(Character %in% Ep1Characters$Character) |&gt;\n  group_by(Character) |&gt;\n  mutate(Count = str_count(Line, \"\\\\b\\\\w+\\\\b\")) |&gt;\n  mutate(NumLines = n()) |&gt;\n  summarise(AvgCount = mean(Count), NumLines = NumLines) |&gt;\n  arrange(desc(AvgCount)) |&gt;\n  distinct()\n\nAvgWordsSpoken\n\n# A tibble: 16 × 3\n# Groups:   Character [16]\n   Character               AvgCount NumLines\n   &lt;chr&gt;                      &lt;dbl&gt;    &lt;int&gt;\n 1 Documentary Crew Member    19           3\n 2 Todd Packer                17.6        74\n 3 Michael                    15.3     11806\n 4 Dwight                     12.6      7393\n 5 Ryan                       11.0      1324\n 6 Jan                        10.9       919\n 7 Jim                        10.4      6666\n 8 Pam                        10.0      5264\n 9 Angela                      9.90     1677\n10 Oscar                       9.87     1464\n11 Stanley                     9.68      750\n12 Michel                      9.6         5\n13 Roy                         9.56      255\n14 Kevin                       9.44     1678\n15 Phyllis                     9.00     1054\n16 Man                         8.86       44\n\n\nThese results display that there is a fairly wide range of average sentence lengths. There also seems to be a correlation between speaking often and speaking a lot. There are, of course some exceptions to this rule, but it is particularly true for Michael Scott.\nQuestion 3:\nWho has the longest ’um’s?\n\nUmTracker &lt;- df |&gt;\n  group_by(Character) |&gt;\n  mutate(umInstances = str_extract(Line, regex(\"\\\\bum+\\\\b\", ignore_case = TRUE))) |&gt;\n  filter(!is.na(umInstances)) |&gt;\n  mutate(umLength = nchar(umInstances)) |&gt;\n  summarise(maxUmLength = max(umLength), maxUmLine = Line[which.max(umLength)], .groups = \"drop\") |&gt;\n  select(Character, maxUmLength, maxUmLine) |&gt;\n  arrange(desc(maxUmLength))\n\nUmTracker\n\n# A tibble: 73 × 3\n   Character maxUmLength maxUmLine                                              \n   &lt;chr&gt;           &lt;int&gt; &lt;chr&gt;                                                  \n 1 Pam                 5 \" Ummmm…   \"                                           \n 2 Darryl              4 \" [freezes] Ummm… [a moment later] Alright. Obviously …\n 3 David               4 \"   Ummm… okay, here’s the thing though.  The plan doe…\n 4 Donna               4 \" Ummm, no.\"                                           \n 5 Jim                 4 \" Ummm… no idea.\"                                      \n 6 Kelly               4 \" Ummm, like a week ago, we got really wasted and it j…\n 7 Michael             4 \" Ummm-hmmm…\"                                          \n 8 Oscar               4 \" Ummm… \"                                              \n 9 Andy                3 \" Umm, on the contrary. \"                              \n10 Angela              3 \" I have a very important announcement to make… about……\n# ℹ 63 more rows\n\n\nPam has the longest um! In Season 2, Episode 9, she has a line in which all she says is “Ummmm…”. It makes sense for Pam’s character to have the longest ‘um’ considering how reserved she is. This reservedness reveals itself in the previous plot in that she has a relatively small average sentence length relative to the number of lines she speaks. In fact, of all of the characters that have more than 2,000 lines, she is the only one with an average length less than 10. Being unsure of herself, it makes sense that Pam would be the only character to have this long of a hesitation written into the script.\nQuestion 4:\nWhat words come before exclamation marks and question marks?\n\nWowWords &lt;- df |&gt;\n  mutate(instance = str_to_lower(str_extract(Line, \"\\\\b\\\\w+(?=!)\"))) |&gt;\n  filter(!is.na(instance)) |&gt;\n  group_by(instance) |&gt;\n  summarize(count = n(), .groups = \"drop\") |&gt;\n  arrange(desc(count))\n\nQuestionWords &lt;- df |&gt;\n  mutate(instance = str_to_lower(str_extract(Line, \"\\\\b\\\\w+(?=\\\\?)\"))) |&gt;\n  filter(!is.na(instance)) |&gt;\n  group_by(instance) |&gt;\n  summarize(count = n(), .groups = \"drop\") |&gt;\n  arrange(desc(count))\n\nPlot 1:\n\nTenWowWords &lt;- WowWords |&gt;\n  head(10)\n  \n\nggplot(TenWowWords, aes(x = instance, y = count)) +\n  geom_bar(stat = \"identity\", fill = \"dodgerblue\") +\n  coord_flip() +\n  labs(title = \"10 Most Frequent Words Before '!'\", x = \"Words\", y = \"Count\") +\n  theme_minimal()\n\n\n\n\n\n\n\n\nThis plot displays the 10 words that most often appear immediately before an exclamation mark. My favorite aspect of this plot is that for most of the words, you can imagine who said them and how. For example, just as the viewer may have guessed, Dwight said ‘Michael!’ 29 times over the course of the show, more than twice as much as any other character. Similarly, Michael ended his sentences with ‘no!’ and ‘god!’ much more often than any other character. It is also important to note that these words differ significantly from the list of 10 most used words in general.\nPlot 2:\n\nTenQuestionWords &lt;- QuestionWords |&gt;\n  head(10)\n  \n\nggplot(TenQuestionWords, aes(x = instance, y = count)) +\n  geom_bar(stat = \"identity\", fill = \"dodgerblue\") +\n  coord_flip() +\n  labs(title = \"10 Most Frequent Words Before '?'\", x = \"Words\", y = \"Count\") +\n  theme_minimal()\n\n\n\n\n\n\n\n\nThis plot displays the 10 words that most often appear immediately before a question mark. Unlike the previous plot, it is difficult to imagine who typically said each word and how. However, it is still quite interesting, though not necessarily surprisingly, that there is one word, “what”, that precedes a question mark significantly more often than any other. It is also important to note that these words differ significantly from the list of 10 most used words in general.\nPlot 3:\n\nWowWords &lt;- df |&gt;\n  mutate(instance = str_to_lower(str_extract(Line, \"\\\\b\\\\w+(?=!)\"))) |&gt;\n  filter(!is.na(instance)) |&gt;\n  group_by(instance) |&gt;\n  summarize(count_wow = n(), .groups = \"drop\")\n\nQuestionWords &lt;- df |&gt;\n  mutate(instance = str_to_lower(str_extract(Line, \"\\\\b\\\\w+(?=\\\\?)\"))) |&gt;\n  filter(!is.na(instance)) |&gt;\n  group_by(instance) |&gt;\n  summarize(count_question = n(), .groups = \"drop\")\n\ncommonWords &lt;- full_join(WowWords, QuestionWords, by = \"instance\") |&gt;\n  mutate(\n    count_wow = ifelse(is.na(count_wow), 0, count_wow),         \n    count_question = ifelse(is.na(count_question), 0, count_question), \n    total_wow = sum(count_wow),                                 \n    total_question = sum(count_question),                       \n    prop_wow = count_wow / total_wow,                \n    prop_question = count_question / total_question   \n  ) |&gt;\n  arrange(desc(prop_wow + prop_question)) |&gt;\n  head(10) \n\nggplot(commonWords, aes(x = prop_wow, y = prop_question, label = instance)) +\n  geom_point(stat = \"identity\", color = \"#1f77b4\", size = 3) +\n  geom_text(vjust = -0.25, hjust = -0.5, size = 4, color = \"black\") + \n  labs(\n    title = \"Relative Frequency of Words Before '?' and '!'\",\n    subtitle = \"Top 10 most frequent words\",\n    x = \"Proportion Before '!'\",\n    y = \"Proportion Before '?'\"\n  ) +\n  theme_minimal(base_size = 15) + \n  theme(\n    legend.position = \"none\",\n    axis.title = element_text(size = 14),\n    axis.text = element_text(size = 12),\n    plot.title = element_text(size = 18, face = \"bold\"),\n    plot.subtitle = element_text(size = 14, face = \"italic\")\n  )\n\n\n\n\n\n\n\n\nThis graph plots the relative frequencies of the 10 most common words used before exclamation marks and question marks. Some words, such as ‘you’ and ‘it’ precede each sign with similar frequencies. Most others, including ‘what’, ‘no’, ‘oh’, ‘hey’, and ‘yes’ differ significantly in their proportions. Among these, ‘what’ is the biggest outlier because it precedes nearly 1% of all question marks in the show! No other word appears before either of the signs with even half that frequency.\nData Source\nThe dataset used in this analysis is The Office Lines Dataset. It contains transcripts from all episodes of The Office (U.S.), including character dialogue, season, episode number, and other metadata.\nPlace this section at the bottom of your document, right after your last analysis and discussion. Let me know if you want help formatting it differently!"
  }
]