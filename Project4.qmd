---
title: "Data Ethics"
description: |
  The Ethics of Mosaicked Deidentification
author: Jack Susank
date: April 15, 2025
format: html
execute:
  warning: false
  message: false
---

There exists a growing problem in the field of data science which pertains to how data is sourced. Useful data oftentimes comes from individuals, but there are a whole host of problems that could arise when this data can be linked to the person it describes. Medical data for example, can contain very personal details about a person's life. In the wrong hands, these details could be used to profit off of the individual and potentially at their expense. The process of eliminating the possibility of the data being reconnected with the person it describes, is called de-identification, and it may appear that removing names from a dataset is enough to guarantee this, but this is not the case! It turns out that there are several other details that could be used as identifiers and "the HIPAA Privacy Rule designates 18" of them (NashBio). Unfortunately, removing this identifiers is oftentimes not enough to de-identify the data. This is because other quasi-identifiers can be used in conjunction with one another to piece together a person's (or community's) identity. The "concept of combing datasets to fill in the blanks is known as 'mosaicking'" (Forbes) and as an example, we can look at the famous New York City taxi database and the research performed by grad student Anthony Tockar. When the taxi dataset was released in 2013, it was 'anonymized' by performing a mathematical operation (called MD5 hashing) on the license and medallion numbers of each cab driver. This would have sufficiently scrambled these identifiers if the size of the input space wasn't so small.Unfortunately, it was, and because the researchers did not do any salting or keying, people like Tockar (Forbes) and Vijay Pandurangan (ArsTechnica) were able to reverse engineer the original values. Combining this knowledge with the vast numbers of pictures of celebrities in cabs published by the paparazzi everyday, they were then able to match the pictures with their corresponding datatable entry. The cabs and their plate and medallion numbers are clearly depicted in many of these pictures, so by combining this information with the picture's metadata (like time and location), Tockar was easily able to identify the celebrities, their destinations, as well as how much they paid and tipped for each ride. Examples like this reveal how difficult it can be to de-identify data, but luckily other solutions exist. Data managers are able to perform a variety of data manipulation techniques including clustering data into groups, providing row averages instead of individual values, and even producing 'synthetic data' that randomizes the data while maintaining the same distribution within each dimension and capturing inter-dimensional correlations. Approaches such as these allow researchers to draw meaningful conclusions while maintaining a strict de-identification status.

Response Questions (New York City Taxi Dataset):

1.  Were the data made publicly available?

Yes, the dataset was made available to the public by New York City officials in 2013 and it contained detailed information about over 173 million taxi rides (ArsTechnica). The observational units were cab rides and each one included information on trip routes, trip times, and 'anonymized' identifiers for drivers and their vehicles. Although the identifying information was hashed using a method called MD5, the full 20GB dataset was downloadable by anyone and was susceptible to de-anonymization. This level of public accessibility, combined with the flawed de-identification method, meant that sensitive details about drivers and their work patterns were effectively released to the public.

2.  Is the data identifiable? All of it? Some of it? In what way? Are the data sufficiently anonymized or old to be free of ethical concerns? Is anonymity guaranteed?

Unfortunately, despite the intentions of the researchers, the data are identifiable. Each of the table entries can be successfully de-anonymized because although the dataset managers attempted to anonymize it using one-way MD5 hashes on the medallion and license numbers, the structure of those numbers was relatively restricted and therefore susceptible to cryptographic de-anonymization given some time (ArsTechnica). This small input space made it trivial for someone like software developer Vijay Pandurangan to generate all possible hashes and match them against the released dataset. Consequently, anonymity was not guaranteed in any sense. The poor choice of MD5 hashing without keying or salting raises significant ethical concerns and rendered the data highly vulnerable to re-identification. Because the dataset enabled the public to track specific drivers' behaviors, work habits, and potentially even home locations, its release constitutes a serious breach of privacy and being released in 2013, it is not old enough to warrant exemptions for these kind of mistakes (Forbes).

3.  Is the data being used in unintended ways to the original study?

Yes, the data were absolutely used in unintended ways. While the original intent behind releasing the dataset may have been to support transparency within the government and promote research or innovation around public transportation, the poor anonymization opened the door for other intentions. Not only were the cab drivers travel and earnings patterns exposed, but if you ever had a way of knowing the plate number/medallion and the time and location of specific person's cab ride, then you also had access to information like their destination and how much they tipped (Forbes)! Surely, this was not the intention of the city officials, but regardless, it is on them to eliminate any possibility of things like this happening. This specific repurping of the data extended far beyond the intended use and exposes how data, even when altered or partially scrubbed to the extent that it seems unidentifiable, can still be misused (NashBio).

4.  Did they use data to improve life for their users, customers, organizations, and communities?

While the intentions behind the release of the data may have been benevolent, and in general, we must recognize that a move towards transparency within the government is positive, the data did not end up effectively serving to improve life for users, drivers, or communities. Particularly due to its lack of privacy, the ethical concerns generated by the flawed anonymization method overshadow any possible benefits that the data might have offered. Instead of boosting transparency and promoting public transportation (such as by fostering efficiency gains) within communities, the release exposed drivers to potential scrutiny, surveillance, and exploitation, which undoubtedly could do more harm than good (ArsTechnica).



## Works Cited

“De-Identification: Balancing Privacy and Utility in Healthcare Data.” *Nashville Biosciences*, 24 Jan. 2025, [nashbio.com/blog/healthcare-data/de-identification-balancing-privacy-and-utility-in-healthcare-data/](https://nashbio.com/blog/healthcare-data/de-identification-balancing-privacy-and-utility-in-healthcare-data/).

Goodin, Dan. “Poorly Anonymized Logs Reveal NYC Cab Drivers’ Detailed Whereabouts.” *Ars Technica*, 23 June 2014, [https://arstechnica.com/tech-policy/2014/06/poorly-anonymized-logs-reveal-nyc-cab-drivers-detailed-whereabouts/](https://arstechnica.com/tech-policy/2014/06/poorly-anonymized-logs-reveal-nyc-cab-drivers-detailed-whereabouts/).

Leetaru, Kalev. “The Big Data Era of Mosaicked Deidentification: Can We Anonymize Data Anymore?” *Forbes*, 24 Aug. 2016, [https://www.forbes.com/sites/kalevleetaru/2016/08/24/the-big-data-era-of-mosaicked-deidentification-can-we-anonymize-data-anymore/](https://www.forbes.com/sites/kalevleetaru/2016/08/24/the-big-data-era-of-mosaicked-deidentification-can-we-anonymize-data-anymore/).

